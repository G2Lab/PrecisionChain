{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "insertData-variantPerson.py\n",
    "Inserts data from VCF files (variant data)\n",
    "Usage: $ python insertData.py -cn=<chain name> -dr=<Chain path> -vf=<VCF path>\n",
    "modified by AE 03/2023\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import binascii\n",
    "import argparse\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "import multiprocessing\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import random\n",
    "import itertools\n",
    "from io import BytesIO\n",
    "from itertools import islice\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#Given a chain name and the name of the new stream, subscribe to that stream\n",
    "def subscribeToStream(chainName, streamName, multichainLoc, datadir):\n",
    "    subscribeStreamCommand=multichainLoc+'multichain-cli {} -datadir={} subscribe {}'.format(chainName,datadir,streamName)\n",
    "    #subscribe to the stream\n",
    "    procSubscribe = subprocess.Popen(subscribeStreamCommand.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    procSubscribe.wait()\n",
    "    return\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#subscribe to person-variant streams\n",
    "def subscribeToStreams(chainName, multichainLoc, datadir):\n",
    "    for i in range(1, 23):\n",
    "        subscribeToStream(chainName, \"person_chrom_{}\".format(i), multichainLoc, datadir)\n",
    "        \n",
    "    subscribeToStream(chainName, \"mappingData_variants\", multichainLoc, datadir)\n",
    "    return\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def loadFilePaths(dataPath, variantFiles):\n",
    "    '''\n",
    "    Load the VCF file paths that will be inserted\n",
    "    Input:\n",
    "        dataPath - path where the VCF files are stored\n",
    "        variantFiles - files to be added (one per chromosome)\n",
    "    '''\n",
    "    #parse the files that are part of user input\n",
    "    if variantFiles == 'all':\n",
    "        files = [x for x in range(1,23)]\n",
    "    else:\n",
    "        files = str.split(variantFiles.replace(\" \",\"\"), ',')\n",
    "    paths = []    \n",
    "    for file in files:\n",
    "        filePath= f'{dataPath}/chr_{file}.vcf.gz'\n",
    "        paths.append(filePath)\n",
    "    random.shuffle(paths)\n",
    "    return paths\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def metadataPerson(metaFile, variantFile):\n",
    "    '''\n",
    "    load metadata associated with the samples in the variant file\n",
    "    Input:\n",
    "        metaFile - path where the metadata is held\n",
    "        variantFiles - files to be added (one per chromosome)\n",
    "    '''\n",
    "    \n",
    "    #extract samples from vcf file\n",
    "    request = 'bcftools query -l {}'.format(variantFile)\n",
    "    output = subprocess.check_output(request, shell = True)\n",
    "    samples = output.decode().split()\n",
    "    #metadata\n",
    "    meta = pd.read_csv(f'{metaFile}')\n",
    "    meta['id'] = meta['id'].astype(str)\n",
    "    #get dict in the same order as the vcf file\n",
    "    meta_seq = meta[meta['id'].isin(samples)]\n",
    "    return meta_seq, samples\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "def extractPersonVariants(file, sample_id):\n",
    "    '''\n",
    "    Extract all variants for patient and wrangle to format for blockchain data entry\n",
    "    Input:\n",
    "        sample_id: id of the sample being added\n",
    "    '''\n",
    "    ##BCFtools request\n",
    "    filter_stmt = '''awk '{if($10 != \"0|0\" && $10 != \"0/0\") { print } }' '''\n",
    "    request = f'''bcftools view -s {sample_id}  -H  {file}| head -100 | {filter_stmt} '''\n",
    "    output = subprocess.check_output(request, shell = True)\n",
    "    ##extract the data from the output\n",
    "    df = pd.read_csv(BytesIO(output), sep='\\t', usecols = [0,1,3,4,9], skiprows = 1, names= ['chrom', 'pos', 'ref', 'alt', 'gt'], index_col = 'pos')\n",
    "    ##clean the ./. genotype\n",
    "    df['gt'] = df['gt'].str[:3]\n",
    "    df['gt'] =  df['gt'].str.replace('.','0')\n",
    "    df.index = df.index.map(str)\n",
    "    try:\n",
    "        #streamname\n",
    "        chrom = df['chrom'].iloc[0]\n",
    "        #remove homoz genotypes\n",
    "        df = df[df['gt'] != '0|0']\n",
    "        ##reorder the genotypes\n",
    "        def gt_parser(row):\n",
    "            if int(row['gt'][0]) < int(row['gt'][2]):\n",
    "                row['gt'] = f\"{row['gt'][2]}|{row['gt'][0]}\"\n",
    "            return row\n",
    "        df = df.apply(gt_parser, axis = 1)\n",
    "        #dict format to add\n",
    "        values = df[['ref','alt', 'gt']].T.to_dict(orient = 'list')\n",
    "        return chrom, values\n",
    "    except:\n",
    "        chrom = file.split('/')[-1].split('.')[0]\n",
    "        return chrom , False\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "def chunkDictionary(values_dict, SIZE=2000):\n",
    "    '''\n",
    "    chunk the person variant dictionary as its too large for a single entry\n",
    "    Input:\n",
    "        values_dict: person variant dictionary from extractPersonVariants\n",
    "    '''\n",
    "    def chunks(values_dict, SIZE=2000):\n",
    "        it = iter(values_dict)\n",
    "        for i in range(0, len(values_dict), SIZE):\n",
    "            yield {k:values_dict[k] for k in islice(it, SIZE)}\n",
    "    \n",
    "    split_variants = {}\n",
    "    for i, chunk in enumerate(chunks(values_dict, SIZE = 2000)):\n",
    "        split_variants[i] = chunk\n",
    "    return split_variants\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "def publishToDataStream(chainName, multichainLoc, datadir, streamName, streamKeys, streamValues):\n",
    "    '''\n",
    "    Publish person entry to field\n",
    "    Input:\n",
    "        streamName: chromosome being added\n",
    "        streamKeys: person_id\n",
    "        streamValues: all the variants:genotype for that person \n",
    "    '''\n",
    "    publishCommand = [multichainLoc+'multichain-cli', \n",
    "        str('{}'.format(chainName)), \n",
    "        str('-datadir={}'.format(datadir)),\n",
    "        'publish',\n",
    "        str('person_chrom_{}'.format(streamName)), \n",
    "        str('{}'.format(streamKeys)),\n",
    "        str('{}'.format(streamValues))]\n",
    "    #procPublish = subprocess.Popen(publishCommand, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    #procPublish.wait()\n",
    "    print(publishCommand)\n",
    "    return\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "def publishMappingPerson(chainName, multichainLoc, datadir, meta):\n",
    "    '''\n",
    "    Publish the samples that were added during this insertion\n",
    "    Input:\n",
    "        samples: List of all sample ID in file\n",
    "    '''\n",
    "    samples = list(meta['id'].values)\n",
    "    streamName = 'mappingData_variants'\n",
    "    streamKeys = 'samples'\n",
    "    streamValues = '{'+'\"json\":{}'.format(json.dumps(samples)) + '}'\n",
    "    publishCommand = [multichainLoc+'multichain-cli', \n",
    "        str('{}'.format(chainName)), \n",
    "        str('-datadir={}'.format(datadir)),\n",
    "        'publish',\n",
    "        str('{}'.format(streamName)), \n",
    "        str('{}'.format(streamKeys)),\n",
    "        str('{}'.format(streamValues))]\n",
    "    \n",
    "    \n",
    "    procPublish = subprocess.Popen(publishCommand, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    procPublish.wait()\n",
    "    \n",
    "    ## person mapping\n",
    "    for i, row in meta.iterrows():\n",
    "        streamName = 'mappingData_variants'\n",
    "        streamKeys = f\"{row['id']}, {row['company']}, {row['seq_machine']}, {row['seq_protocol']}, {str(row['coverage'])}, {row['alignment_protocol']}, {row['variant_calling']}\"\n",
    "        streamValues = '{'+'\"json\":{}'+ '}'\n",
    "        publishCommand = [multichainLoc+'multichain-cli', \n",
    "            str('{}'.format(chainName)), \n",
    "            str('-datadir={}'.format(datadir)),\n",
    "            'publish',\n",
    "            str('{}'.format(streamName)), \n",
    "            str('{}'.format(streamKeys)),\n",
    "            str('{}'.format(streamValues))]\n",
    "        procPublish = subprocess.Popen(publishCommand, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        procPublish.wait()\n",
    "\n",
    "    ## by sequence\n",
    "    for seq in meta['sequence'].unique():\n",
    "        s = list(meta['id'][meta['sequence'] == seq])\n",
    "        streamName = 'mappingData_variants'\n",
    "        streamKeys = f'{seq}'\n",
    "        streamValues = '{'+'\"json\":{}'.format(json.dumps(s)) + '}'\n",
    "        publishCommand = [multichainLoc+'multichain-cli', \n",
    "            str('{}'.format(chainName)), \n",
    "            str('-datadir={}'.format(datadir)),\n",
    "            'publish',\n",
    "            str('{}'.format(streamName)), \n",
    "            str('{}'.format(streamKeys)),\n",
    "            str('{}'.format(streamValues))]\n",
    "\n",
    "        procPublish = subprocess.Popen(publishCommand, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        procPublish.wait()\n",
    "    return\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "def publishToDataStreams(fields):\n",
    "    chainName, multichainLoc, datadir, metaFile, paths, people = fields\n",
    "    '''\n",
    "    loop through all samples and add to multichain\n",
    "    Input:\n",
    "        mappingFile - path where the person:vcf ID mapping is held\n",
    "        paths - path for the VCF files being added\n",
    "    '''\n",
    "    for variantFile in paths:\n",
    "        #load mapping dictionary and file paths\n",
    "        meta, samples = metadataPerson(metaFile, variantFile)\n",
    "        publishMappingPerson(chainName, multichainLoc, datadir, meta)\n",
    "        for sample_id in samples:\n",
    "            streamName, streamValues = extractPersonVariants(variantFile, sample_id)\n",
    "            ## only submit if have non ./. alleles\n",
    "            if isinstance(streamValues, dict):\n",
    "                ##chunk the dictionary as too large for one entry\n",
    "                split_variants = chunkDictionary(streamValues, SIZE=200)\n",
    "                for v in split_variants.values():\n",
    "                    streamKeys = sample_id\n",
    "                    streamValues ='{'+'\"json\":{}'.format(json.dumps(v)) +'}'#create JSON data object\n",
    "                    publishToDataStream(chainName, multichainLoc, datadir, streamName, streamKeys, streamValues)\n",
    "            else:\n",
    "                pass\n",
    "                    \n",
    "        print('Inserted {}'.format(variantFile))\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainName, multichainLoc, datadir = '', '', ''\n",
    "geneFile = '/gpfs/commons/groups/gursoy_lab/aelhussein/blockchain/code/16k_scalability/data/gtf/gtf_chr1.txt' \n",
    "variantFile = '/gpfs/commons/groups/gursoy_lab/aelhussein/blockchain/code/16k_scalability/data/vcf/beadchip/chr_1.vcf.gz' \n",
    "metaFile='/gpfs/commons/groups/gursoy_lab/aelhussein/blockchain/code/16k_scalability/data/mapping_vocab/metadata.csv'\n",
    "people = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mapping dictionary and file paths\n",
    "meta, samples = metadataPerson(metaFile, variantFile)\n",
    "#publishMappingPerson(chainName, multichainLoc, datadir, meta)\n",
    "for sample_id in samples:\n",
    "    streamName, streamValues = extractPersonVariants(variantFile, sample_id)\n",
    "    ## only submit if have non ./. alleles\n",
    "    if isinstance(streamValues, dict):\n",
    "        ##chunk the dictionary as too large for one entry\n",
    "        split_variants = chunkDictionary(streamValues, SIZE=200)\n",
    "        for v in split_variants.values():\n",
    "            streamKeys = sample_id\n",
    "            streamValues ='{'+'\"json\":{}'.format(json.dumps(v)) +'}'#create JSON data object\n",
    "            publishToDataStream(chainName, multichainLoc, datadir, streamName, streamKeys, streamValues)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print('Inserted {}'.format(variantFile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = variantFile\n",
    "filter_stmt = '''awk '{if($10 != \"0|0\" && $10 != \"0/0\") { print } }' '''\n",
    "request = f'''bcftools view -s {sample_id}  -H  {file} | head -10000 | {filter_stmt}'''\n",
    "output = subprocess.check_output(request, shell = True)\n",
    "##extract the data from the output\n",
    "df = pd.read_csv(BytesIO(output), sep='\\t', usecols = [0,1,3,4,9], skiprows = 1, names= ['chrom', 'pos', 'ref', 'alt', 'gt'], index_col = 'pos')\n",
    "##clean the ./. genotype\n",
    "#df['gt'] = df['gt'].str[:3]\n",
    "#df['gt'] =  df['gt'].str.replace('.','0')\n",
    "#df.index = df.index.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>818161</th>\n",
       "      <td>1</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>1/0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864083</th>\n",
       "      <td>1</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>1/1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911428</th>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>1/0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929375</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>1/1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938178</th>\n",
       "      <td>1</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>1/1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86285432</th>\n",
       "      <td>1</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>1/1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86319121</th>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>1/0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86367970</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>1/1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86442466</th>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>0/1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86444020</th>\n",
       "      <td>1</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>1/0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4979 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          chrom ref alt   gt\n",
       "pos                         \n",
       "818161        1   G   A  1/0\n",
       "864083        1   T   C  1/1\n",
       "911428        1   C   T  1/0\n",
       "929375        1   A   G  1/1\n",
       "938178        1   G   T  1/1\n",
       "...         ...  ..  ..  ...\n",
       "86285432      1   T   G  1/1\n",
       "86319121      1   C   A  1/0\n",
       "86367970      1   A   G  1/1\n",
       "86442466      1   C   T  0/1\n",
       "86444020      1   T   C  1/0\n",
       "\n",
       "[4979 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f5b56c13b7484c00a55994cdba89dd694e797803c89a7a509cdf6820a3c2bac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
